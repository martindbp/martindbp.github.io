{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNKaJz5j_ylj",
    "tags": [
     "remove"
    ]
   },
   "source": [
    "# \"Can I Say This In Chinese\" with BERT (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJR6t_gCQe_x"
   },
   "source": [
    "## Introduction\n",
    "As it turns out, most people are not very inclined to teaching. I'm learning Chinese, my wife *is* Chinese, seems like a match made in heaven. Except that she has no patience whatsoever with my broken Chinese (though she's wonderful in many other ways). Whenever I ask how to say something in Chinese, she anwers with either \"I don't know\" or \"you can't say that (followed by no explanation)\". The only way I can get anything out of her is by trying to say something in Chinese and asking whether it sounds right or not. This is less mentally taxing for her than actually having to translate from English, which I understand, especially for two languages so dissimilar.\n",
    "\n",
    "Now I'm thinking, with the recent advances in Natural Language Processing with Deep Learning, maybe I can create something to replace my unwilling wife. The academic name for this task seems to be \"Linguistic Acceptability\". Exactly what this includes seems to be up for debate.  For example, \"the mouse ate the cat\" is perfectly grammatical, although highly unlikey. Then there are sentences which are grammatical but seem logically impossible, like \"the cat is a bus\". This sentence makes no sense *unless* you've watched the movie Totoro, which features a... cat that is also a bus. Since this seems like a very difficult problem, I'll be focusing more on distinguishing grammatical vs. ungrammatical rather than sensical vs. nonsensical.\n",
    "\n",
    "My hope is that using a model that can tell us whether a sentence is grammatical or not, we can use it not only for that purpose, but also for *generating* grammatical sentences in interesting ways such as correcting an ungrammatical sentence or generating sentences with specific words in them.\n",
    "\n",
    "## Defining the problem\n",
    "Recent Deep Learning architectures like BERT and GPT-2 basically train a *language model* or LM, i.e. given the surrounding context, they try to predict the missing word. In GPT-2s case, it predicts the next word given all the *previous* words in the sentence, while BERT predicts a missing word (a cloze) given both the words before and after it (the B in BERT stands for bidirectional). As such, GPT-2 works better as a language model, defining the joint probability over a sequence of words, while BERT's masked LM is less straight forward to use as such. As a reminder, the joint probability can be refactored recursively using the chain rule:\n",
    "\n",
    "$$P(w_{1:n}) = P(w_n | w_{1:n-1})P(w_{1:n-1}) = P(w_n | w_{1:n-1}) \\cdot \\ldots \\cdot P(w_2 | w_1)P(w_1)$$\n",
    "\n",
    "Each of these factors is exactly what we get out of GPT-2, which means if we run inference and multiply the factors we get the joint probability of the whole sentence. BERT on the other hand gives us $P(w_k | w_{1:k-1}, w_{k+1:n})$ which is harder to intepret. There is research exploring ways of getting a joint probability model out of BERT [using MRFs (Markov Random Fields)](https://www.aclweb.org/anthology/W19-2304), but I'd like to keep things simple for this little project.\n",
    "\n",
    "Using GPT-2 will be difficult, since training it from scratch, having 1.5 *billion* weights, requires a [cluster of GPUs and roughly $50k](https://blog.usejournal.com/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc). So I'm constrained to pre-trained versions, of which there is none for Chinese AFAIK. The Python library pytorch-transformers *does* however have a pre-trained BERT for Chinese.\n",
    "\n",
    "### How can we use BERT?\n",
    "Being constrained by time and money leaves me no option but to use BERT at this point. While BERT can't be used as a language model per-se, we can perhaps use the output in some useful way.\n",
    "\n",
    "We'd like to get a binary decision whether a sentence is acceptable or not. We could try to use the masked probability for each word in the sentence, but again, it will be difficult to find some absolute thresold to distinguish unlikely sentences from unacceptable ones. What we could do is to train a classifier based on BERT with a dataset of positive and negative examples. While there are such datasets for other languages (CoLA - Corpus of Linguistic Acceptablility), I have not found such a dataset for Chinese.\n",
    "\n",
    "I was however able to crawl some examples from the [AllSet grammar wiki](https://resources.allsetlearning.com/chinese/grammar/) (licensed with CC-NC) with in total 436 and 461 negative and positive examples respectively, split into grammar groups based on page (note: this will take some time to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1691,
     "status": "ok",
     "timestamp": 1571056322780,
     "user": {
      "displayName": "Martin Pettersson",
      "photoUrl": "",
      "userId": "08400294021953718221"
     },
     "user_tz": -120
    },
    "id": "kZYyFOBPa7CU",
    "outputId": "b4da89ff-a7fb-41ba-ba37-8bfe73a37888",
    "tags": [
     "remove"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# For caching calculations and pytorch models, we can save them to Google Drive.\n",
    "# By running the code below, we can mount our drive to the file system. Running\n",
    "# this cell prints a login URL which produces an authorization code to be entered.\n",
    "cache_gdrive = True\n",
    "cache_path = ''\n",
    "if cache_gdrive:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  cache_path = '/content/gdrive/My Drive/Colab Notebooks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_u5JcyaDgUi",
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "! wget --quiet --mirror --convert-links --adjust-extension --follow-tags=a --no-parent resources.allsetlearning.com/chinese/grammar/\n",
    "! grep -r -e 'class=\"x\"' resources.allsetlearning.com/chinese/**/* |\\\n",
    "  sed -e 's/<li class=\"x\">//g' -e 's/<span .*//g' -e 's/<\\/*[a-z]*>//g' -e 's/ //g' -e 's/:.*→/:/g' \\\n",
    "  > \"$cache_path/allset_negative_examples.txt\"\n",
    "! grep -r -e 'class=\"o\"' resources.allsetlearning.com/chinese/**/* |\\\n",
    "  sed -e 's/<li class=\"o\">//g' -e 's/<span .*//g' -e 's/<\\/*[a-z]*>//g' -e 's/ //g' -e 's/:.*→/:/g' \\\n",
    "  > \"$cache_path/allset_positive_examples.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_Ul0xRfJuJC"
   },
   "source": [
    "While it's putting the car before the horse a bit, I suspected (correctly) that this small dataset would not be enough to train a classifier that generalizes well to any output. There are just too few examples to generalize to all the ways sentences can be correct and wrong, although these examples do contain many important and subtle errors learners commit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcNjuGPiCbCJ"
   },
   "source": [
    "## Self-supervised learning\n",
    "\n",
    "Instead of only training on the small dataset, the idea is to pre-train a classifier in a self-supervised way by generating negative examples from positive ones. While the masked probabilities of all the words in a sentence is not enough to tell the acceptability of the sentence, we can assume there is useful information in the *relative* scores, or losses, between sentences.\n",
    "\n",
    "Using relative losses, we can generate negative samples from positive ones by finding a *mutation* that significantly increases the loss. Let's define the loss for a sentence as the average (since we're possibly comparing sentences of differing lengths) Cross-Entropy loss for each word: \n",
    "$$\n",
    "L(S) = -\\frac{1}{N}\\sum_{i=1}^{N}{\\log(P(w_i  | w_{1:i-1}, w_{i+1:N}))}\n",
    "$$\n",
    "Then we can perform take a correct sentence $S_c$ and perform a random mutation to get $S_m$. If $L(S_m) - L(S_c) > \\epsilon$ we consider it to be unacceptable. \n",
    "\n",
    "Note that even if we could use the bidirectional probabilities/losses to directly do classification, this is something we'd like to avoid since calculating this loss requires a forward pass for *every* token in the sentence. Using these expensively generated examples to train a classifier let's us bypass this problem. \n",
    "\n",
    "### Hard Negatives\n",
    "This way we can generate unacceptable sentences from any acceptable one. Now since there are many possible ways to mutate a sentence that increases the loss more than $\\epsilon$, we can pick the minimal one that passes this threshold. This is similar to *hard negative mining* where if you already have a model, you can improve it by sampling hard negatives and retraining the model. This is common in image classification and localization where any part of an image *not* containing the specified object are potential negative examples. Then it makes sense to pick the ones that are misclassified or get high losses from the initial model.\n",
    "\n",
    "### Mutations\n",
    "For the actual sentences, we could use the original corpus, but I prefer using sentences from [Tatoeba](tatoeba.org) since it is a good source of informal language suitable for learners.\n",
    "\n",
    "For mutating the sentences, there are a few things we can do:\n",
    "* Permute the words\n",
    "* Swap two words\n",
    "* Insert word (sampled based on corpus frequency)\n",
    "* Replace word (sampled based on corpus frequency)\n",
    "* Delete word\n",
    "\n",
    "While we want to mutate the sentences to get unacceptable ones, there is some degree of unacceptability, and we want to generate ones that are *hard*, i.e. just barely unacceptable. Therefore I exclude random permutations since they are very unlikely to produce something close to acceptability.\n",
    "\n",
    "Similarly for insertions and word replacements, it makes more sense to sample common words more frequently than rare words since the language has a very long tail of very infrequent words. \n",
    "\n",
    "Below is the code for loading the Tatoeba dataset and generating hard negatives.\n",
    "(NOTE: this is *a lot* of not very interesting code, but it is runnable if you run this in a Jupyter Notebook or Google Colab environment). Also worth mentioning is that the starting point for the PyTorch training was this [Colab Notebook](https://mccormickml.com/2019/07/22/BERT-fine-tuning/), which serves as a good tutorial for fine-tuning BERT for sequence classification.\n",
    "\n",
    "First, installing some pip packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0NmMdkZO8R6q",
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install --quiet pytorch-transformers pytorch-nlp hanziconv jieba sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LzL7BG9Gpbed"
   },
   "source": [
    "Import a pre-trained Masked LM BERT model and define functions for preparing data for this model, as well as functions for predicting based on it, and calculating losses for whole sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ok002ceNB8E7",
    "tags": [
     "toggle_input",
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import jieba\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm, trange\n",
    "from hanziconv import HanziConv\n",
    "from sympy.ntheory import factorint\n",
    "from functools import lru_cache\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split, GroupKFold\n",
    "from pytorch_transformers import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_transformers import AdamW, BertForSequenceClassification, BertForMaskedLM\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "from sklearn.metrics import matthews_corrcoef, precision_score, recall_score, accuracy_score\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "def gpu_usage(print_stats=False):\n",
    "  \"\"\" Convenience function to check GPU memory usage. Returns free memory in GB \"\"\"\n",
    "  nvmlInit()\n",
    "  handle = nvmlDeviceGetHandleByIndex(0)\n",
    "  info = nvmlDeviceGetMemoryInfo(handle)\n",
    "  if print_stats:\n",
    "    print(f\"Total memory: {info.total/1e9:.2f} GB\")\n",
    "    print(f\"Free memory: {info.free/1e9:.2f} GB\")\n",
    "    print(f\"Used memory: {info.used/1e9:.2f} GB\")\n",
    "  return info.free/1e9\n",
    "\n",
    "# Make sure we have enough memory\n",
    "if gpu_usage(print_stats=True) < 8:\n",
    "  raise SystemError('Not enough memory')\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese', do_lower_case=True)\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "masked_lm_model = BertForMaskedLM.from_pretrained('bert-base-chinese')\n",
    "masked_lm_model.cuda()\n",
    "\n",
    "def prepare_data(df, test_size=0.1, batch_size=32, shuffle=True, add_cls_sep=True):\n",
    "  sentences = df.sentence.values\n",
    "  # We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "  if add_cls_sep:\n",
    "    sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "  has_labels = 'label' in df.columns\n",
    "  if has_labels:\n",
    "    labels = df.label.values\n",
    "  else:\n",
    "    labels = np.zeros(len(sentences))\n",
    "\n",
    "  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "  # Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "  # In the original paper, the authors used a length of 512.\n",
    "  MAX_LEN = 128\n",
    "\n",
    "  # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "  # Pad our input tokens\n",
    "  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "  # Create attention masks\n",
    "  attention_masks = []\n",
    "\n",
    "  # Create a mask of 1s for each token followed by 0s for padding\n",
    "  for seq in input_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "  # Use train_test_split to split our data into train and validation sets for training\n",
    "  # but if test_size is zero then only generate training sets\n",
    "  if test_size > 0.0:\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
    "        input_ids, labels, random_state=2018, test_size=test_size, shuffle=shuffle)\n",
    "    train_masks, validation_masks, _, _ = train_test_split(\n",
    "        attention_masks, input_ids, random_state=2018, test_size=test_size, shuffle=shuffle)\n",
    "  else:\n",
    "    train_inputs = input_ids\n",
    "    train_labels = labels\n",
    "    train_masks = attention_masks\n",
    "    validation_inputs = []\n",
    "    validation_labels = []\n",
    "    validation_masks = []\n",
    "    \n",
    "  # Convert all of our data into torch tensors, the required datatype for our model\n",
    "  train_inputs = torch.tensor(train_inputs)\n",
    "  validation_inputs = torch.tensor(validation_inputs)\n",
    "  train_labels = torch.tensor(train_labels)\n",
    "  validation_labels = torch.tensor(validation_labels)\n",
    "  train_masks = torch.tensor(train_masks)\n",
    "  validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "  # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "  # with an iterator the entire dataset does not need to be loaded into memory\n",
    "  train_data = TensorDataset(train_inputs, train_masks, *([train_labels] if has_labels else []))\n",
    "  if shuffle:\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "  else:\n",
    "    train_sampler = SequentialSampler(train_data)\n",
    "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "  validation_data = TensorDataset(validation_inputs, validation_masks, *([validation_labels] if has_labels else []))\n",
    "  validation_sampler = SequentialSampler(validation_data)\n",
    "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "  return train_dataloader, validation_dataloader\n",
    "\n",
    "\n",
    "def predict(dataloader, model, has_labels=True):\n",
    "  \"\"\"\n",
    "  Evaluates data from a data loader on a model and returns either a tuple of\n",
    "  predicted probability and true label if has_labels=True otherwise it returns\n",
    "  the raw logits\n",
    "  \"\"\"\n",
    "  # Put model in evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  # Predict \n",
    "  for i, batch in enumerate(dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    if has_labels:\n",
    "      b_input_ids, b_input_mask, b_labels = batch\n",
    "    else:\n",
    "      b_input_ids, b_input_mask = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits, *_ = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    if has_labels:\n",
    "      softmax_probs = np.exp(logits[:, 1]) / np.exp(logits).sum(axis=1)\n",
    "      label_ids = b_labels.to('cpu').numpy()\n",
    "      for prob, label in zip(softmax_probs, label_ids):\n",
    "        yield prob, label\n",
    "    else:\n",
    "      yield logits\n",
    "\n",
    "  \n",
    "def eval_loss_sentences(sentences, masking='char'):\n",
    "  \"\"\"\n",
    "  Evaluate the loss for a list of sentences\n",
    "  sentences: the list of sentences\n",
    "  masking: 'word' for whole word, and 'char' for single character masking\n",
    "  \"\"\"\n",
    "  assert masking in ['word', 'char']\n",
    "  masking_words = masking == 'word'\n",
    "  indexed_sentence_tokens = []\n",
    "  tokenized_sentences = []\n",
    "  sentence_mask_indices = []\n",
    "  all_examples = []\n",
    "  for sentence in sentences:\n",
    "    # NOTE: the tokenizer removes spaces\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "    tokenized_sentence = tokenized_sentence[:128]\n",
    "    indexed_sentence_tokens.append(tokenizer.convert_tokens_to_ids(tokenized_sentence))\n",
    "    if masking_words:\n",
    "      tokenized_sentence = list(t[0] for t in jieba.tokenize(''.join(tokenized_sentence)))\n",
    "    tokenized_sentences.append(tokenized_sentence)\n",
    "    mask_indices = []\n",
    "    char_idx = 0\n",
    "    for i in range(len(tokenized_sentence)):\n",
    "      mask_token = tokenized_sentence[i]\n",
    "      mask_token_parts = len(tokenizer.tokenize(mask_token)) if masking_words else 1\n",
    "      all_examples.append(''.join(tokenized_sentence[:i]) +\n",
    "                          ''.join(mask_token_parts*['[MASK]']) +\n",
    "                          ''.join(tokenized_sentence[i+1:]))\n",
    "      mask_indices.append((char_idx, char_idx+mask_token_parts))\n",
    "      char_idx += mask_token_parts\n",
    "    mask_indices.append('[SEP]')\n",
    "    sentence_mask_indices.append(mask_indices)\n",
    "\n",
    "  df = pd.DataFrame(data={'sentence': all_examples})\n",
    "  dataloader, _ = prepare_data(df, test_size=0.0, batch_size=32, shuffle=False)\n",
    "\n",
    "  sentence_losses = []\n",
    "  curr_sentence_loss = 0\n",
    "  curr_sentence = 0\n",
    "  curr_mask_idx = 0\n",
    "  curr_example = 0\n",
    "  for batch_logits in predict(dataloader, masked_lm_model, has_labels=False):\n",
    "    for i in range(batch_logits.shape[0]):\n",
    "      mask_start, mask_end = sentence_mask_indices[curr_sentence][curr_mask_idx]\n",
    "      for m in range(mask_start, mask_end):\n",
    "        mask_logits = batch_logits[i][m+1]\n",
    "        mask_logits_exp = np.exp(mask_logits)\n",
    "        mask_token_probs = mask_logits_exp / mask_logits_exp.sum()\n",
    "        mask_entropy = -(mask_token_probs * np.log(mask_token_probs)).sum()\n",
    "        masked_token_index = indexed_sentence_tokens[curr_sentence][m]\n",
    "        # Cross-Entropy Loss\n",
    "        curr_sentence_loss += -np.log(mask_token_probs[masked_token_index])\n",
    "\n",
    "      curr_mask_idx += 1\n",
    "      curr_example += 1\n",
    "      if curr_mask_idx == len(tokenized_sentences[curr_sentence]):\n",
    "        # We've reached a new sentence, reset and append log prob\n",
    "        # Normalize sentence loss by number of tokens\n",
    "        curr_sentence_loss /= len(tokenized_sentences[curr_sentence])\n",
    "        sentence_losses.append(curr_sentence_loss)\n",
    "        curr_sentence_loss = 0\n",
    "        curr_mask_idx = 0\n",
    "        curr_sentence += 1\n",
    "  return sentence_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUNRih90p2ga"
   },
   "source": [
    "Download example sentences from Tatoeba and word frequency dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlGS0WkaTUpZ",
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "! wget http://downloads.tatoeba.org/exports/sentences.tar.bz2\n",
    "! bzip2 -dc sentences.tar.bz2 > \"$cache_path/sentences.txt\"\n",
    "! wget https://www.plecoforums.com/download/weibo_wordfreq-release_utf-8-txt.2603 -O \"$cache_path/weibo.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9IIV5MFv4zC"
   },
   "source": [
    "Below is the code for reading the Tatoeba and Weibo frequency datasets and generating hard negatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSxtct3QOLKs",
    "tags": [
     "toggle_input",
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "orig_sentences = []\n",
    "with open(cache_path+'/sentences.txt', 'r') as f:\n",
    "  for line in f:\n",
    "      splits = line.split('\\t')\n",
    "      if len(splits) < 3:\n",
    "        continue\n",
    "      _, lang, zh = line.split('\\t')\n",
    "      if lang != 'cmn': continue\n",
    "      zh = HanziConv.toSimplified(zh.strip())\n",
    "      orig_sentences.append(zh)\n",
    "\n",
    "words = []\n",
    "counts = []\n",
    "with open(cache_path+'/weibo.txt', 'r', encoding='utf-8-sig') as f: \n",
    "    for line in f.readlines():\n",
    "        word, count = line.split('\\t')\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        if len(tokenized_word) == 0:\n",
    "          continue\n",
    "        \n",
    "        # Skip [UNK] or other garbage unkown to the BERT tokenizer\n",
    "        skip = False\n",
    "        for t in tokenized_word:\n",
    "          if len(t) > 1:\n",
    "            skip = True\n",
    "            break\n",
    "        if skip: continue\n",
    "        words.append(word)\n",
    "        counts.append(int(count))\n",
    "\n",
    "# Calculate the probability and cumulative probability function for words over\n",
    "# the frequency\n",
    "counts = np.array(counts)\n",
    "word_probs = counts / counts.sum()\n",
    "cdf = np.cumsum(word_probs)\n",
    "\n",
    "def sample_word():\n",
    "  \"\"\" Sample a random word based on frequency \"\"\"\n",
    "  r = random.random()\n",
    "  idx = np.searchsorted(cdf, r)\n",
    "  return words[idx]\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def middle_coprime(n):\n",
    "  \"\"\" Find the middle coprime of a number, e.g. of all the\n",
    "      sorted coprimes of n, pick the middle one \"\"\"\n",
    "  factors = list(factorint(n).keys())\n",
    "  coprimes = [1]\n",
    "  for i in range(n-2, 1, -1):\n",
    "    coprime = True\n",
    "    for f in factors:\n",
    "      if i % f == 0:\n",
    "        coprime = False\n",
    "        break\n",
    "    if coprime:\n",
    "      coprimes.append(i)\n",
    "  return coprimes[len(coprimes) // 2]\n",
    "\n",
    "def pseudo_random_range(from_idx, to_idx=None):\n",
    "  \"\"\"\n",
    "  Visit all indices in a range pseudo-randomly by visiting (ax + b) mod n, \n",
    "  where a and n are co-prime. Small and large coprimes tend to not look random,\n",
    "  so pick the middle one.\n",
    "  \"\"\"\n",
    "  if to_idx is None:\n",
    "    from_idx, to_idx = 0, from_idx\n",
    "\n",
    "  n = to_idx - from_idx\n",
    "  coprime = middle_coprime(n)\n",
    "  offset = random.randint(0, n-1) if n > 1 else 0\n",
    "  for i in range(0, n):\n",
    "    yield from_idx + (coprime*i + offset) % n \n",
    "\n",
    "\n",
    "IGNORE = set(['。', '」', '「', '，', ' ', '！', '？', '?', '!', '.', ','])\n",
    "# Swaps that usually produce acceptable sentences:\n",
    "POSITIVE_SWAP_GROUPS = [set(['我', '你', '他', '她']), # personal pronouns\n",
    "                       set(['我们', '你们', '他们', '她们'])] # plural personal pronouns\n",
    "def is_positive_swap(from_token, to_token):\n",
    "  swap_set = set([from_token, to_token])\n",
    "  # Check if both tokens are in a positive swap group, if so we don't swap\n",
    "  for swap_group in POSITIVE_SWAP_GROUPS:\n",
    "    if len(swap_set & swap_group) == 2:\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "def generate_delete(sentence, tokens):\n",
    "  for idx in pseudo_random_range(len(tokens)):    \n",
    "    token = tokens[idx][0]\n",
    "    if token in IGNORE:\n",
    "      continue\n",
    "    tokens_deleted = tokens[:idx] + tokens[idx+1:]\n",
    "    yield ''.join(t[0] for t in tokens_deleted)\n",
    "\n",
    "def generate_insert(sentence, tokens):\n",
    "  for idx in pseudo_random_range(len(tokens)):    \n",
    "    word = sample_word()\n",
    "    tokens_inserted = tokens[:idx] + [(word,)] + tokens[idx:]\n",
    "    yield ''.join(t[0] for t in tokens_inserted)\n",
    "\n",
    "def generate_replace(sentence, tokens):\n",
    "  for idx in pseudo_random_range(len(tokens)):    \n",
    "    token = tokens[idx][0]\n",
    "    if token in IGNORE:\n",
    "      continue\n",
    "    # Sample words until it's not equal to the token we're replacing\n",
    "    word = token\n",
    "    while word == token:\n",
    "      word = sample_word()\n",
    "    tokens_replaced = tokens[:idx] + [(word,)] + tokens[idx+1:]\n",
    "    yield ''.join(t[0] for t in tokens_replaced)\n",
    "\n",
    "def generate_swap(sentence, tokens):\n",
    "  token_set = set([t[0] for t in tokens])\n",
    "  for from_idx in pseudo_random_range(len(tokens)-1):    \n",
    "    from_token = tokens[from_idx][0]\n",
    "    if from_token in IGNORE:\n",
    "      continue\n",
    "\n",
    "    for to_idx in pseudo_random_range(from_idx, len(tokens)):\n",
    "      to_token = tokens[to_idx][0]\n",
    "      if (from_token == to_token or\n",
    "          to_token in IGNORE):\n",
    "          continue\n",
    "\n",
    "      if is_positive_swap(from_token, to_token):\n",
    "        continue\n",
    "        \n",
    "      # Swap the tokens and return the new string\n",
    "      mtokens = list(tokens)\n",
    "      mtokens[to_idx], mtokens[from_idx] = mtokens[from_idx], mtokens[to_idx]\n",
    "      yield ''.join(t[0] for t in mtokens)\n",
    "\n",
    "def generate_mutated(sentence):\n",
    "  tokens = list(jieba.tokenize(sentence))\n",
    "  generators = [#generate_delete(sentence, tokens),\n",
    "                generate_insert(sentence, tokens),\n",
    "                generate_replace(sentence, tokens),\n",
    "                generate_swap(sentence, tokens)]\n",
    "  pick_probs = np.array([0.15, 0.15, 0.7])\n",
    "  while len(generators) > 0:\n",
    "    gen_idx = np.random.choice(np.arange(len(generators)), p=pick_probs)\n",
    "    random_gen = generators[gen_idx]\n",
    "    try:\n",
    "      yield next(random_gen)\n",
    "    except StopIteration:\n",
    "      # The generator is out of sentences to generate, so remove it\n",
    "      del generators[gen_idx]\n",
    "      pick_probs = np.delete(pick_probs, gen_idx)\n",
    "      # Need to normalize so probabilities add up to 1\n",
    "      pick_probs /= pick_probs.sum()\n",
    "\n",
    "\n",
    "def generate_hard_negatives(sentences, model, loss_threshold=0.5, generate_max=10,\n",
    "                            debug_print=False):\n",
    "  \"\"\"\n",
    "  Creates hard negative examples, which are sampled based on mutations that\n",
    "  increase the loss the least but still significantly enough to very likely be a\n",
    "  true negative.\n",
    "  \"\"\"\n",
    "  sentence_examples = list(sentences)\n",
    "  for i, sentence in enumerate(sentence_examples):\n",
    "    # Skip sentences with unknown words or other garbage\n",
    "    predict_sentences = [sentence]\n",
    "    generator = generate_mutated(sentence)\n",
    "    for _ in range(generate_max):\n",
    "      try:\n",
    "        predict_sentences.append(next(generator))\n",
    "      except StopIteration:\n",
    "        break\n",
    "    \n",
    "    losses = eval_loss_sentences(predict_sentences, masking='char')\n",
    "    print('C: ', sentence)\n",
    "    for s, l in sorted(zip(predict_sentences[1:], losses[1:]), key=lambda x: x[1]):\n",
    "      if l - losses[0] > loss_threshold:\n",
    "        if debug_print:\n",
    "          print('W: ', s, l, ' +', l-losses[0])\n",
    "        yield s\n",
    "        break\n",
    "\n",
    "\n",
    "negatives_path = cache_path + '/negatives.txt'\n",
    "if os.path.exists(negatives_path):\n",
    "  with open(negatives_path, 'r') as f:\n",
    "    hard_negatives = [l.strip() for l in f.readlines()]\n",
    "else:\n",
    "  # NOTE: generating hard negatives takes a long time since to check a single mutation\n",
    "  # we need to run inference len(sentence) times, and we need to generate a number\n",
    "  # of mutations for each sentence in order to find a good one\n",
    "  # So we run a few thousand at a time and store them in case runtime gets recycled\n",
    "  use_num = 30000\n",
    "  num_at_a_time = 3000\n",
    "  use_sentences = orig_sentences[:use_num]\n",
    "  hard_negatives = []\n",
    "  for i in range(0, use_num // num_at_a_time):\n",
    "    if os.path.exists(f'{cache_path}/negatives{i+1}.txt'):\n",
    "      continue\n",
    "    with open(f'{cache_path}/negatives{i+1}.txt', 'w') as f:\n",
    "      sentences = use_sentences[i*num_at_a_time:(i+1)*num_at_a_time]\n",
    "      for negative in generate_hard_negatives(sentences, masked_lm_model, debug_print=True):\n",
    "        hard_negatives.append(negative)\n",
    "        f.write(negative + '\\n')\n",
    "\n",
    "  # Concatenate all files to one\n",
    "  with open(negatives_path, 'w') as n:\n",
    "    for i in range(0, use_num // num_at_a_time):\n",
    "      with open(f'{cache_path}/negatives{i+1}.txt', 'r') as f:\n",
    "        n.write(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fPHer7mmmxv6"
   },
   "source": [
    "## Fine-tuning BERT\n",
    "There are plenty of tutorials on how to fine-tune a BERT model. For this experiment I'll use the pre-trained Chinese model in the Python library [pytorch-transformers](https://github.com/huggingface/transformers) by huggingface. This model is trained with a character-by-character tokenizer, meaning multi-character Chinese words are split into separate word embeddings for each character. This may be [suboptimal](https://github.com/ymcui/Chinese-BERT-wwm/blob/master/README_EN.md), unless the model is powerful enough to capture the structure of words, but for now this is what we have to work with.\n",
    "\n",
    "Below is the code for training and validating the BERT model for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6J-FYdx6nFE_",
    "tags": [
     "toggle_input",
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "def train(dataloader, epochs=4, model=None, debug_print=False):\n",
    "  if model is None:\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels=2);\n",
    "    model.cuda()\n",
    "\n",
    "  param_optimizer = list(model.named_parameters())\n",
    "  no_decay = ['bias', 'gamma', 'beta']\n",
    "  optimizer_grouped_parameters = [\n",
    "      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "       'weight_decay_rate': 0.01},\n",
    "      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "       'weight_decay_rate': 0.0}\n",
    "  ]\n",
    "  optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "  train_loss_set = []\n",
    "\n",
    "  # trange is a tqdm wrapper around the normal python range which prints progress\n",
    "  r = trange(epochs, desc=\"Epoch\") if debug_print else range(epochs)\n",
    "  for _ in r:\n",
    "    # Tracking variables\n",
    "    train_loss = 0\n",
    "    num_examples, num_steps = 0, 0\n",
    "\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in enumerate(dataloader):\n",
    "      if debug_print: print(f'Batch: {step}')\n",
    "      # Add batch to GPU\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      # Unpack the inputs from our dataloader\n",
    "      b_input_ids, b_input_mask, b_labels = batch\n",
    "      # Clear out the gradients (by default they accumulate)\n",
    "      optimizer.zero_grad()\n",
    "      # Forward pass\n",
    "      loss, *_ = model(b_input_ids, token_type_ids=None,\n",
    "                       attention_mask=b_input_mask, labels=b_labels)\n",
    "      train_loss_set.append(loss.item())    \n",
    "      # Backward pass\n",
    "      loss.backward()\n",
    "      # Update parameters and take a step using the computed gradient\n",
    "      optimizer.step()\n",
    "\n",
    "      # Update tracking variables\n",
    "      train_loss += loss.item()\n",
    "      num_examples += b_input_ids.size(0)\n",
    "      num_steps += 1\n",
    "\n",
    "    if debug_print: print(\"Train loss: {}\".format(train_loss/num_steps))\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, df=None):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for prob, label in predict(dataloader, model):\n",
    "      y_true.append(label)\n",
    "      y_pred.append(1 if prob > 0.5 else 0)\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def print_stats(y_true, y_pred, sentences=None, label=None):\n",
    "  tab = ''\n",
    "  if label is not None:\n",
    "    print(f'{label}:')\n",
    "    tab = '\\t'\n",
    "  print(f'{tab}Matthews Correlaton Coefficient:', matthews_corrcoef(y_true, y_pred))\n",
    "  print(f'{tab}Accuracy:', accuracy_score(y_true, y_pred))\n",
    "  print(f'{tab}Precision:', precision_score(y_true, y_pred))\n",
    "  print(f'{tab}Recall:', recall_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epOadVCcyJ2g"
   },
   "source": [
    "Now we can train our first classification model on positive examples from the Tatoeba dataset and our generated hard negatives. Here I'll train the classifier with an increasing number of examples to see if we need more data. Training an iterating is slow, so I prefer to keep it as small as possible for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 394973,
     "status": "ok",
     "timestamp": 1571056727585,
     "user": {
      "displayName": "Martin Pettersson",
      "photoUrl": "",
      "userId": "08400294021953718221"
     },
     "user_tz": -120
    },
    "id": "MzEAS26doTjf",
    "outputId": "f0e6778d-e906-4063-fda8-7eb6d318ff0c",
    "tags": [
     "toggle_input",
     "remove_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final:\n",
      "\tMatthews Correlaton Coefficient: 0.9743148993944\n",
      "\tAccuracy: 0.9871181129776307\n",
      "\tPrecision: 0.9810055865921787\n",
      "\tRecall: 0.9934720167116372\n"
     ]
    }
   ],
   "source": [
    "model_path = cache_path + '/self_supervised_classification_model.pt'\n",
    "if os.path.exists(model_path):\n",
    "  classification_model = torch.load(model_path)\n",
    "else:\n",
    "  training_accuracies = []\n",
    "  validation_accuracies = []\n",
    "  classification_model = None\n",
    "  for num in [3000, 6000, 9000, len(hard_negatives)]:\n",
    "    hard_negatives_df = pd.DataFrame(data={\n",
    "        'sentence': hard_negatives[:num] + orig_sentences[num:2*num],\n",
    "        'orig': orig_sentences[:2*num],\n",
    "        'label': num*[0]+num*[1]})\n",
    "\n",
    "    train_dataloader, validation_dataloader = prepare_data(\n",
    "        hard_negatives_df, test_size=0.1, batch_size=32)\n",
    "    \n",
    "    classification_model = train(train_dataloader, epochs=4, debug_print=False)\n",
    "    print('Train accuracy: ', accuracy(*evaluate(classification_model, train_dataloader)))\n",
    "    print('Validation accuracy: ', accuracy(*evaluate(classification_model, validation_dataloader)))\n",
    "  \n",
    "  # Save to disk, for rerunning and making copies\n",
    "  torch.save(classification_model, model_path)\n",
    "\n",
    "df = pd.DataFrame(data={\n",
    "    'sentence': hard_negatives + orig_sentences[len(hard_negatives):2*len(hard_negatives)],\n",
    "    'label': len(hard_negatives)*[0] + len(hard_negatives)*[1]})\n",
    "dataloader, _ = prepare_data(df, test_size=0.0, batch_size=32)\n",
    "print_stats(*evaluate(classification_model, dataloader), label='Final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "acYnHe6w78fi"
   },
   "source": [
    "Now let's load the AllSet grammatical wiki examples and train models with cross-validation either from scratch or using the pre-trained model.\n",
    "\n",
    "One important difference from the previous dataset is that we want to know how well the model generalizes to new *unseen* grammatical rules rather than just unseen examples. Therefore we split the data into training and validation sets based on the grammatical rule/group, such that examples from the same group never are split between the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iIPs5F0h9do-",
    "tags": [
     "toggle_input",
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "allset_negative_examples = defaultdict(list)\n",
    "with open(cache_path+'/allset_negative_examples.txt', 'r') as f:\n",
    "  for l in f.readlines():\n",
    "    filename, sentence = l.split(':')\n",
    "    allset_negative_examples[filename].append(sentence.strip())\n",
    "allset_positive_examples = defaultdict(list)\n",
    "with open(cache_path+'/allset_positive_examples.txt', 'r') as f:\n",
    "  for l in f.readlines():\n",
    "    filename, sentence = l.split(':')\n",
    "    allset_positive_examples[filename].append(sentence.strip())\n",
    "\n",
    "all_files = list(set(allset_negative_examples.keys()) |\n",
    "                 set(allset_positive_examples.keys()))\n",
    "allset_sentences = []\n",
    "allset_labels = []\n",
    "allset_groups = []\n",
    "for g, filename in enumerate(all_files):\n",
    "  negative = allset_negative_examples[filename]\n",
    "  positive = allset_positive_examples[filename]\n",
    "  allset_sentences += negative + positive\n",
    "  allset_labels += [0]*len(negative) + [1]*len(positive)\n",
    "  allset_groups += (len(negative)+len(positive))*[g]\n",
    "\n",
    "allset_sentences = np.array(allset_sentences)\n",
    "allset_labels = np.array(allset_labels)\n",
    "allset_groups = np.array(allset_groups)\n",
    "\n",
    "tatoeba_sample = np.random.choice(orig_sentences, 10000)\n",
    "hard_negative_sample = np.random.choice(hard_negatives, 10000)\n",
    "self_supervised_df = pd.DataFrame(data={\n",
    "    'sentence': list(hard_negative_sample) + list(tatoeba_sample),\n",
    "    'label': len(hard_negative_sample)*[0] + len(tatoeba_sample)*[1]})\n",
    "self_supervised_dataloader, _ = prepare_data(self_supervised_df, test_size=0.0,\n",
    "                                             batch_size=32, shuffle=False)\n",
    "\n",
    "def cross_validate_allset(initial_model_path=None, epochs=4, n_splits=10,\n",
    "                          print_progress=True):\n",
    "  train_results = [[], []]\n",
    "  test_results = [[], []]\n",
    "  self_supervised_results = [[], []]\n",
    "  new_model = None\n",
    "  if n_splits == 1:\n",
    "    generator = [(np.arange(len(allset_sentences)),\n",
    "                 np.arange(len(allset_sentences)))]\n",
    "  else:\n",
    "    group_kfold = GroupKFold(n_splits=n_splits)\n",
    "    generator = group_kfold.split(allset_sentences, allset_labels, allset_groups)\n",
    "\n",
    "  for i, (train_index, test_index) in enumerate(generator):\n",
    "    train_examples = allset_sentences[train_index]\n",
    "    train_labels = allset_labels[train_index]\n",
    "    test_examples = allset_sentences[test_index]\n",
    "    test_labels = allset_labels[test_index]\n",
    "  \n",
    "    train_dataloader, _ = prepare_data(\n",
    "        pd.DataFrame(data={'sentence': train_examples, 'label': train_labels}),\n",
    "        test_size=0.0, batch_size=32)\n",
    "    test_dataloader, _ = prepare_data(\n",
    "        pd.DataFrame(data={'sentence': test_examples, 'label': test_labels}),\n",
    "        test_size=0.0, batch_size=32)\n",
    "  \n",
    "    model = None\n",
    "    if initial_model_path is not None:\n",
    "      model = torch.load(initial_model_path)\n",
    "\n",
    "    new_model = train(train_dataloader, epochs=epochs, model=model,\n",
    "                      debug_print=print_progress)\n",
    "    \n",
    "    train_result = evaluate(new_model, train_dataloader)\n",
    "    test_result = evaluate(new_model, test_dataloader)\n",
    "    self_supervised_result = evaluate(new_model, self_supervised_dataloader)\n",
    "    if print_progress:\n",
    "      print_stats(*train_result, label='AllSet Train')\n",
    "      print_stats(*test_result, label='AllSet Test')\n",
    "      print_stats(*self_supervised_result, label='Self-Supervised')\n",
    "\n",
    "    train_results[0] += train_result[0]\n",
    "    train_results[1] += train_result[1]\n",
    "    test_results[0] += test_result[0]\n",
    "    test_results[1] += test_result[1]\n",
    "    self_supervised_results[0] += self_supervised_result[0]\n",
    "    self_supervised_results[1] += self_supervised_result[1]\n",
    "  \n",
    "  print_stats(*train_result, label='Overall AllSet Train')\n",
    "  print_stats(*test_result, label='Overall AllSet Test')\n",
    "  print_stats(*self_supervised_result, label='Overall Self-Supervised')\n",
    "\n",
    "  # Return the last model\n",
    "  return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPTNzJ2AIFQd"
   },
   "source": [
    "First, let's train a model from scratch on the AllSet data and see how well it does against against itself as well as against our self-supervised Tatoeba + Hard negative dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5102560,
     "status": "ok",
     "timestamp": 1570719529602,
     "user": {
      "displayName": "Martin Pettersson",
      "photoUrl": "",
      "userId": "08400294021953718221"
     },
     "user_tz": -120
    },
    "id": "laWnJaMiHNZR",
    "outputId": "c7c1ae58-cef0-485e-c96d-4cf19ff1c6c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AllSet Train:\n",
      "\tMatthews Correlaton Coefficient: 0.978298651254621\n",
      "\tAccuracy: 0.9891304347826086\n",
      "\tPrecision: 0.9838337182448037\n",
      "\tRecall: 0.9953271028037384\n",
      "Overall AllSet Test:\n",
      "\tMatthews Correlaton Coefficient: 0.9366607354497857\n",
      "\tAccuracy: 0.967391304347826\n",
      "\tPrecision: 0.94\n",
      "\tRecall: 1.0\n",
      "Overall Self-Supervised:\n",
      "\tMatthews Correlaton Coefficient: 0.46815654446892113\n",
      "\tAccuracy: 0.7165\n",
      "\tPrecision: 0.6568613244457325\n",
      "\tRecall: 0.9066\n"
     ]
    }
   ],
   "source": [
    "cross_validate_allset(initial_model_path=None, epochs=6, n_splits=10, print_progress=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PJzDjQXIVrS"
   },
   "source": [
    "As you can see, it seems to generalize well on the AllSet data across the folds, meaning somehow it generalizes to unseen grammatical rules. But the performance on the self-supervised dataset is poor. This is probably due to the AllSet data being biased towards easier, illustrative examples, which are substantially different from the average sentence from Tatoeba. It also doesn't cover all the more \"obvious\" ways sentences can be grammatical.\n",
    "\n",
    "Now lets do the same thing, but with a model pre-trained on the self-supervised dataset, with the hope that we can generalize on both data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5072230,
     "status": "ok",
     "timestamp": 1570724601855,
     "user": {
      "displayName": "Martin Pettersson",
      "photoUrl": "",
      "userId": "08400294021953718221"
     },
     "user_tz": -120
    },
    "id": "QS7nC1O_IyRy",
    "outputId": "f017cf3f-8abe-4af6-9ee4-e4d024c708dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AllSet Train:\n",
      "\tMatthews Correlaton Coefficient: 0.9927488225424451\n",
      "\tAccuracy: 0.9963768115942029\n",
      "\tPrecision: 0.9976580796252927\n",
      "\tRecall: 0.9953271028037384\n",
      "Overall AllSet Test:\n",
      "\tMatthews Correlaton Coefficient: 0.9784719757905218\n",
      "\tAccuracy: 0.9891304347826086\n",
      "\tPrecision: 0.9791666666666666\n",
      "\tRecall: 1.0\n",
      "Overall Self-Supervised:\n",
      "\tMatthews Correlaton Coefficient: 0.8895640148971811\n",
      "\tAccuracy: 0.94365\n",
      "\tPrecision: 0.9777107785075912\n",
      "\tRecall: 0.908\n"
     ]
    }
   ],
   "source": [
    "cross_validate_allset(initial_model_path=model_path, epochs=6, n_splits=10, print_progress=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fn4M8uZRJMrj"
   },
   "source": [
    "The overall results show that the model has generalized relatively well to both datasets, although the scores are lower for the self-supervised data set compared to before.\n",
    "\n",
    "For training the final model, we can get an even better result for the self-supervised data by training it from scratch on both data sets, but with the AllSet data upsampled to match the self-supervised in size, giving both equal importance. Here I'll train it once with a single test set instead of k-fold cross validation, so I don't time out in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 640042,
     "status": "ok",
     "timestamp": 1570801524473,
     "user": {
      "displayName": "Martin Pettersson",
      "photoUrl": "",
      "userId": "08400294021953718221"
     },
     "user_tz": -120
    },
    "id": "37WodxMwCsNz",
    "outputId": "8eebe26d-7f83-476c-a027-aecb8e3f6cbb",
    "tags": [
     "toggle_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tMatthews Correlaton Coefficient: 0.9928113288113345\n",
      "\tAccuracy: 0.9963996843558888\n",
      "\tPrecision: 0.9937496995047839\n",
      "\tRecall: 0.9992264926275078\n",
      "AllSet Test:\n",
      "\tMatthews Correlaton Coefficient: 1.0\n",
      "\tAccuracy: 1.0\n",
      "\tPrecision: 1.0\n",
      "\tRecall: 1.0\n",
      "Self-Supervised Test:\n",
      "\tMatthews Correlaton Coefficient: 0.9869869628581678\n",
      "\tAccuracy: 0.9934725848563969\n",
      "\tPrecision: 0.9885462555066079\n",
      "\tRecall: 0.998220640569395\n"
     ]
    }
   ],
   "source": [
    "final_model_path = cache_path+'/final_model.pt' \n",
    "if os.path.exists(final_model_path):\n",
    "  final_model = torch.load(final_model_path)\n",
    "else:\n",
    "  # Again, need to split AllSet into train/test using GroupKFold\n",
    "  # GroupKFold.split returns all cross-validation sets, but we'll just use the first\n",
    "  allset_train_idx, allset_test_idx = next(GroupKFold(n_splits=10).split(allset_sentences, allset_labels, allset_groups))\n",
    "  allset_train = allset_sentences[allset_train_idx]\n",
    "  allset_train_labels = allset_labels[allset_train_idx]\n",
    "  allset_test = allset_sentences[allset_test_idx]\n",
    "  allset_test_labels = allset_labels[allset_test_idx]\n",
    "  \n",
    "  # Next split the self-supervised data set into train/test as well\n",
    "  ss_train, ss_test, ss_train_labels, ss_test_labels =  train_test_split(\n",
    "      orig_sentences[len(hard_negatives):2*len(hard_negatives)] + hard_negatives,\n",
    "      [1]*len(hard_negatives) + [0]*len(hard_negatives), test_size=0.1)\n",
    "  \n",
    "  # Then combine both data sets, but with upsampling for AllSet so that they are\n",
    "  # of equal size\n",
    "  upsample_times = 2*len(hard_negatives) // len(allset_sentences)\n",
    "  all_train = (list(ss_train) + upsample_times*list(allset_train))\n",
    "  all_train_labels = (ss_train_labels + upsample_times*list(allset_train_labels))\n",
    "  \n",
    "  all_train_dataloader, _ = prepare_data(\n",
    "      pd.DataFrame(data={'sentence': all_train, 'label': all_train_labels}),\n",
    "      test_size=0.0, batch_size=32)\n",
    "  allset_test_dataloader, _ = prepare_data(\n",
    "      pd.DataFrame(data={'sentence': allset_test, 'label': allset_test_labels}),\n",
    "      test_size=0.0, batch_size=32)\n",
    "  ss_test_dataloader, _ = prepare_data(\n",
    "      pd.DataFrame(data={'sentence': ss_test, 'label': ss_test_labels}),\n",
    "      test_size=0.0, batch_size=32)\n",
    "  \n",
    "  final_model = train(all_train_dataloader, epochs=4,\n",
    "                      model=torch.load(model_path),\n",
    "                      debug_print=True)\n",
    "  \n",
    "  train_result = evaluate(final_model, all_train_dataloader)\n",
    "  allset_test_result = evaluate(final_model, allset_test_dataloader)\n",
    "  ss_test_result = evaluate(final_model, ss_test_dataloader)\n",
    "  print_stats(*train_result, label='Train')\n",
    "  print_stats(*allset_test_result, label='AllSet Test')\n",
    "  print_stats(*ss_test_result, label='Self-Supervised Test')\n",
    "  torch.save(final_model, final_model_path)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tpxgTIRZ-4B0"
   },
   "source": [
    "And a sanity check on a few new examples I've found by googling, and some I've come up with myself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1238,
     "status": "ok",
     "timestamp": 1570799995464,
     "user": {
      "displayName": "Martin Pettersson",
      "photoUrl": "",
      "userId": "08400294021953718221"
     },
     "user_tz": -120
    },
    "id": "UhCkn_mUtSA3",
    "outputId": "5d3b7d45-3bab-4217-8980-3d640ae1c1ff",
    "tags": [
     "toggle_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct | Incorrect\n",
      "你有没有车: 1.00 | 你有没有车吗？: 0.29\n",
      "你很高: 1.00 | 你是很高: 0.02\n",
      "你的包很漂亮: 1.00 | 你得包很漂亮: 0.00\n",
      "这辆车很贵: 1.00 | 这个车很贵: 1.00\n",
      "这辆车很贵: 1.00 | 这本车很贵: 1.00\n",
      "我昨天在公园碰到他了: 0.87 | 我碰到他在公园昨天了: 0.00\n",
      "在一家中国饭店，马丽和汤姆见面了。: 1.00 | 在一家中国饭店，马丽见面了汤姆。: 0.01\n",
      "他们在法国和对方见面了。: 1.00 | 他们在法国见面了对方。: 0.00\n",
      "马丽嫁了汤姆。: 0.84 | 马丽结婚了汤姆。: 0.00\n",
      "汤姆娶了马丽。: 1.00 | 汤姆结婚了马丽。: 0.00\n",
      "我喜欢所有学生。: 1.00 | 我喜欢都学生。: 0.00\n",
      "这是我的所有。: 1.00 | 这是我的都。: 0.02\n",
      "我们明天上午九点开会。: 1.00 | 我们开会在明天上午九点 。: 0.58\n",
      "我没有时间。: 1.00 | 我不有时间。: 0.00\n"
     ]
    }
   ],
   "source": [
    "incorrect_sentences = [\n",
    "  '你有没有车吗？',\n",
    "  '你是很高',\n",
    "  '你得包很漂亮',\n",
    "  '这个车很贵',\n",
    "  '这本车很贵',\n",
    "  '我碰到他在公园昨天了',\n",
    "  '在一家中国饭店，马丽见面了汤姆。',\n",
    "  '他们在法国见面了对方。',\n",
    "  '马丽结婚了汤姆。',\n",
    "  '汤姆结婚了马丽。',\n",
    "  '我喜欢都学生。',\n",
    "  '这是我的都。',\n",
    "  '我们开会在明天上午九点 。',\n",
    "  '我不有时间。'\n",
    "]\n",
    "correct_sentences = [\n",
    "  '你有没有车',\n",
    "  '你很高',\n",
    "  '你的包很漂亮',\n",
    "  '这辆车很贵',\n",
    "  '这辆车很贵',\n",
    "  '我昨天在公园碰到他了',\n",
    "  '在一家中国饭店，马丽和汤姆见面了。',\n",
    "  '他们在法国和对方见面了。',\n",
    "  '马丽嫁了汤姆。',\n",
    "  '汤姆娶了马丽。',\n",
    "  '我喜欢所有学生。',\n",
    "  '这是我的所有。',\n",
    "  '我们明天上午九点开会。',\n",
    "  '我没有时间。'\n",
    "]\n",
    "\n",
    "incorrect_df = pd.DataFrame(data={'sentence': incorrect_sentences, 'label': len(incorrect_sentences)*[0]})\n",
    "incorrect_dataloader, _ = prepare_data(incorrect_df, test_size=0.0, batch_size=1, shuffle=False)\n",
    "correct_df = pd.DataFrame(data={'sentence': correct_sentences, 'label': len(correct_sentences)*[1]})\n",
    "correct_dataloader, _ = prepare_data(correct_df, test_size=0.0, batch_size=1, shuffle=False)\n",
    "gen = zip(correct_sentences, predict(correct_dataloader, model=final_model, has_labels=True),\n",
    "          incorrect_sentences, predict(incorrect_dataloader, model=final_model, has_labels=True))\n",
    "print('Correct | Incorrect')\n",
    "for correct, (prob_correct, _), incorrect, (prob_incorrect, _) in gen:\n",
    "  print(f'{correct}: {prob_correct:.2f} | {incorrect}: {prob_incorrect:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-qe1zjW_Dpz"
   },
   "source": [
    "For those of you who don't know any Chinese, I'll explain the 3 false positives out of these examples.\n",
    "\n",
    "The first two false positives are when using the wrong \"measure word\" for the noun \"car\". In English we have measure words for some things, like a *pair* of shoes or a *loaf* of bread, but Chinese loads of them. It seems like the model hasn't managed to learn this, but it's also a simple thing to add more data for: we can just find sentences with measure words and swap them for the wrong one.\n",
    "\n",
    "The last error is one of sentence word ordering, where in Chinese the time and place always comes first in a sentence. Getting this wrong is a bit suprising, but it also had a probability of 0.58, so at least it's not very sure about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "--VfCvwr-u4B"
   },
   "source": [
    "# What's next?\n",
    "In the next post in this series, I'll investigate using this model to do some interesting things:\n",
    "1. Given an ungrammatical sentence, run gradient descent on the *input* to find a similar sentence that *is* grammatical\n",
    "2. Find a grammatical sentence of length N which contains a list of specified words\n",
    "3. Do the above, but restrain the sentence to use \"simple\" words, i.e. words with high frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rt_A1S6gelfT",
    "tags": [
     "remove"
    ]
   },
   "outputs": [],
   "source": [
    "def predict_with_gradient(dataloader, model, has_labels=True):\n",
    "  \"\"\"\n",
    "  Evaluates data from a data loader on a model and returns either a tuple of\n",
    "  predicted probability and true label if has_labels=True otherwise it returns\n",
    "  the raw logits\n",
    "  \"\"\"\n",
    "  # Put model in evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "  # Predict \n",
    "  for i, batch in enumerate(dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    if has_labels:\n",
    "      b_input_ids, b_input_mask, b_labels = batch\n",
    "    else:\n",
    "      b_input_ids, b_input_mask = batch\n",
    "\n",
    "    # Forward pass, calculate logit predictions\n",
    "    logits, *_ = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    if has_labels:\n",
    "      softmax_probs = np.exp(logits[:, 1]) / np.exp(logits).sum(axis=1)\n",
    "      label_ids = b_labels.to('cpu').numpy()\n",
    "      for prob, label in zip(softmax_probs, label_ids):\n",
    "        yield prob, label\n",
    "    else:\n",
    "      yield logits\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "can_i_say_this.ipynb",
   "provenance": [
    {
     "file_id": "1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO",
     "timestamp": 1569230717594
    },
    {
     "file_id": "1f_snPs--PVYgZJwT3GwjxqVALFJ0T2-y",
     "timestamp": 1556493831452
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nikola": {
   "date": "2019-10-09 21:09:01 UTC+10:00",
   "previewimage": "images/chinese_placement.png",
   "slug": "can-i-say-this",
   "title": "\"Can I Say This In Chinese\" with BERT (Part 1)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
